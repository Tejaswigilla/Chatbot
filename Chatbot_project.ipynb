{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task**: To build a marketing chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import nltk #natural language toolkit\n",
    "from nltk import bigrams,ngrams,trigrams\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading dataset using pandas and storing as a data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('./dialog_talk_agent(correct).xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting top 5 values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Text Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Tell me about your personality</td>\n",
       "      <td>Just think of me as the ace up your sleeve.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I want to know you better</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Define yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Describe yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>tell me about yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Context  \\\n",
       "0  Tell me about your personality   \n",
       "1       I want to know you better   \n",
       "2                 Define yourself   \n",
       "3               Describe yourself   \n",
       "4          tell me about yourself   \n",
       "\n",
       "                                   Text Response  \n",
       "0    Just think of me as the ace up your sleeve.  \n",
       "1  I can help you work smarter instead of harder  \n",
       "2  I can help you work smarter instead of harder  \n",
       "3  I can help you work smarter instead of harder  \n",
       "4  I can help you work smarter instead of harder  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context          11\n",
       "Text Response    11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping null values if present**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: TEXT NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing libraries for lemmatizing of the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag #for finding parts of speech of words in corpus\n",
    "from nltk.stem import wordnet #for lemmatizing the words in corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing Lemma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lema=wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a function for the normalization of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalize(text):\n",
    "    #Step 1: Converting to lower case\n",
    "    pre_text=text.lower()\n",
    "    #Step 2: Substituting special characters and numbers with empty space\n",
    "    pre_text1=re.sub(r'[^a-z0-9]',' ',pre_text)\n",
    "    #Step 3: Calling pos_tag using pre-processed text\n",
    "    tag_list=pos_tag(nltk.word_tokenize(pre_text1),tagset=None)\n",
    "    lema_sent=[]#initializing empty list\n",
    "    #Step 4: Finding the parts of speech for the processed text\n",
    "    for token,pos_token in tag_list:\n",
    "        if pos_token.startswith('V'):\n",
    "            pos_val='v'\n",
    "        elif pos_token.startswith('R'):#adverb\n",
    "            pos_val='r'\n",
    "        elif pos_token.startswith('J'):#adjective\n",
    "            pos_val='a'\n",
    "        else:#any parts of speech except verb adverb adjective\n",
    "            pos_val='n'\n",
    "        #Step 5: Lemmatizing the word with its POS\n",
    "        lema_token=lema.lemmatize(token,pos_val)#computing\n",
    "        lema_sent.append(lema_token)#append values in list\n",
    "    return ' '.join(lema_sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying the above function to the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Text Response</th>\n",
       "      <th>doc_tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Tell me about your personality</td>\n",
       "      <td>Just think of me as the ace up your sleeve.</td>\n",
       "      <td>tell me about your personality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I want to know you better</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>i want to know you good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Define yourself</td>\n",
       "      <td>I can help you work smarter instead of harder</td>\n",
       "      <td>define yourself</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Context  \\\n",
       "0  Tell me about your personality   \n",
       "1       I want to know you better   \n",
       "2                 Define yourself   \n",
       "\n",
       "                                   Text Response  \\\n",
       "0    Just think of me as the ace up your sleeve.   \n",
       "1  I can help you work smarter instead of harder   \n",
       "2  I can help you work smarter instead of harder   \n",
       "\n",
       "                           doc_tp  \n",
       "0  tell me about your personality  \n",
       "1         i want to know you good  \n",
       "2                 define yourself  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['doc_tp']=df['Context'].apply(text_normalize)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the above function in a pickle** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(text_normalize,open('./text_normalise.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: WORD EMBEDDING\n",
    "It is done using,\n",
    "1. BAG OF WORDS model(BOW)\n",
    "2. TERM FREQUENCY AND INVERSE DOCUMENT FREQUENCY model (TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying bag of words using CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()\n",
    "X=cv.fit_transform(df['doc_tp']).toarray()#converting text to bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting features after applying BOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21', 'abort', 'about', 'absolutely', 'abysmal', 'actually', 'adore', 'advice', 'advise', 'affirmative', 'afraid', 'afternoon', 'again', 'age', 'agree', 'ah', 'ahah', 'ahaha', 'ahahah', 'ahahaha', 'ahead', 'all', 'almost', 'alone', 'already', 'alright', 'alrighty', 'also', 'always', 'amaze', 'amazing', 'an', 'and', 'angry', 'annoy', 'annoying', 'annul', 'answer', 'any', 'anymore', 'anything', 'anytime', 'apologise', 'apologize', 'apology', 'apparently', 'appreciate', 'aren', 'ask', 'asleep', 'assist', 'assistance', 'at', 'attractive', 'aways', 'awesome', 'awful', 'baby', 'back', 'bad', 'be', 'bear', 'beautiful', 'because', 'bed', 'beg', 'best', 'bestie', 'birth', 'birthday', 'bore', 'boring', 'bos', 'bot', 'brainy', 'bravo', 'brilliant', 'buddy', 'busy', 'but', 'bye', 'can', 'cancel', 'care', 'celebrate', 'certainly', 'chat', 'chatbot', 'cheer', 'childhood', 'city', 'clever', 'come', 'confirm', 'cook', 'cookie', 'cool', 'correct', 'could', 'country', 'course', 'crack', 'crazy', 'cry', 'cute', 'cuz', 'date', 'day', 'dear', 'define', 'definitely', 'depress', 'describe', 'disagree', 'discard', 'discuss', 'discussion', 'disgust', 'dismiss', 'disregard', 'do', 'dokie', 'don', 'drain', 'dream', 'each', 'eat', 'enough', 'enrage', 'even', 'evening', 'ever', 'everyone', 'everything', 'exactly', 'excellent', 'excite', 'excited', 'excuse', 'exhaust', 'extremely', 'fake', 'fall', 'fantastic', 'far', 'favor', 'favorite', 'feel', 'few', 'find', 'fine', 'fire', 'fired', 'foot', 'for', 'forget', 'forgive', 'friend', 'friendship', 'from', 'full', 'fun', 'funny', 'furious', 'genius', 'get', 'girl', 'give', 'glad', 'go', 'good', 'goodbye', 'goodnight', 'gorgeous', 'got', 'great', 'greet', 'greeting', 'grieve', 'grow', 'guess', 'guide', 'ha', 'hah', 'haha', 'hahaha', 'hand', 'handsome', 'happen', 'happiness', 'happy', 'have', 'he', 'hear', 'hehe', 'hehehe', 'hello', 'help', 'helpful', 'here', 'hey', 'heya', 'hi', 'hilarious', 'hobby', 'hold', 'home', 'homeland', 'hometown', 'hope', 'horrible', 'horrific', 'house', 'how', 'howdy', 'hug', 'huh', 'human', 'hungry', 'husband', 'idea', 'if', 'in', 'incorrect', 'incredibly', 'indeed', 'insane', 'insomniac', 'insomnious', 'intelligent', 'interested', 'introduce', 'irritate', 'isn', 'it', 'job', 'joke', 'joking', 'just', 'kid', 'kind', 'kinda', 'know', 'knowledge', 'lame', 'later', 'laugh', 'learn', 'leave', 'let', 'life', 'like', 'little', 'live', 'll', 'lmao', 'locate', 'location', 'lol', 'lonely', 'long', 'look', 'lose', 'lot', 'loud', 'love', 'lovely', 'mad', 'make', 'marry', 'marvelous', 'master', 'may', 'me', 'mean', 'meet', 'meeting', 'might', 'mind', 'minute', 'miss', 'moment', 'mood', 'more', 'morning', 'much', 'must', 'my', 'na', 'nah', 'near', 'need', 'never', 'nevermind', 'next', 'nice', 'night', 'no', 'nooo', 'nope', 'not', 'nothing', 'now', 'nut', 'obviously', 'of', 'offer', 'office', 'oh', 'ok', 'okay', 'okey', 'okie', 'old', 'on', 'one', 'only', 'or', 'other', 'our', 'out', 'overload', 'overwork', 'owner', 'pardon', 'people', 'perfect', 'person', 'personality', 'platform', 'play', 'pleasant', 'please', 'pleased', 'pleasure', 'pretty', 'pro', 'problem', 'probs', 'professional', 'program', 'promise', 'qualified', 'question', 're', 'ready', 'real', 'really', 'recommend', 'request', 'residence', 'return', 'right', 'robot', 'rock', 'rush', 'sad', 'say', 'second', 'see', 'seek', 'seem', 'sense', 'shake', 'should', 'shouldn', 'skip', 'sleep', 'sleepless', 'sleepy', 'smart', 'smarter', 'smile', 'so', 'some', 'something', 'soon', 'sorry', 'sound', 'speak', 'special', 'splendid', 'start', 'still', 'stop', 'straight', 'study', 'stuff', 'such', 'suggest', 'suggestion', 'super', 'suppose', 'sure', 'swamp', 'sweet', 'take', 'talk', 'tank', 'tell', 'terrible', 'terrific', 'test', 'thank', 'thanks', 'thanx', 'that', 'the', 'then', 'there', 'thing', 'think', 'this', 'thnx', 'though', 'thrill', 'till', 'time', 'tire', 'tired', 'to', 'today', 'together', 'tomorrow', 'tonight', 'too', 'top', 'totally', 'town', 'true', 'truth', 'unemployed', 'unhappy', 'up', 'upset', 'use', 'useful', 'useless', 've', 'very', 'wait', 'wan', 'want', 'wassup', 'waste', 'way', 'we', 'wear', 'weary', 'weirdo', 'welcome', 'well', 'what', 'whatever', 'whazzup', 'when', 'where', 'which', 'who', 'whole', 'why', 'wife', 'will', 'wise', 'with', 'woah', 'wonderful', 'wooow', 'work', 'world', 'worry', 'worthless', 'would', 'wow', 'wrong', 'xd', 'ya', 'yap', 'ye', 'yea', 'yeah', 'year', 'yeh', 'yep', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yup']\n"
     ]
    }
   ],
   "source": [
    "features=cv.get_feature_names()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting it to a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>21</th>\n",
       "      <th>abort</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abysmal</th>\n",
       "      <th>actually</th>\n",
       "      <th>adore</th>\n",
       "      <th>advice</th>\n",
       "      <th>advise</th>\n",
       "      <th>affirmative</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>yeh</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1576</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1577</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1579</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1581 rows Ã— 491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      21  abort  about  absolutely  abysmal  actually  adore  advice  advise  \\\n",
       "0      0      0      1           0        0         0      0       0       0   \n",
       "1      0      0      0           0        0         0      0       0       0   \n",
       "2      0      0      0           0        0         0      0       0       0   \n",
       "3      0      0      0           0        0         0      0       0       0   \n",
       "4      0      0      1           0        0         0      0       0       0   \n",
       "...   ..    ...    ...         ...      ...       ...    ...     ...     ...   \n",
       "1576   0      0      0           0        0         0      0       0       0   \n",
       "1577   0      0      0           0        0         0      0       0       0   \n",
       "1578   0      0      0           0        0         0      0       0       0   \n",
       "1579   0      0      0           0        0         0      0       0       0   \n",
       "1580   0      0      0           0        0         0      0       0       0   \n",
       "\n",
       "      affirmative  ...  year  yeh  yep  yes  yet  you  your  yours  yourself  \\\n",
       "0               0  ...     0    0    0    0    0    0     1      0         0   \n",
       "1               0  ...     0    0    0    0    0    1     0      0         0   \n",
       "2               0  ...     0    0    0    0    0    0     0      0         1   \n",
       "3               0  ...     0    0    0    0    0    0     0      0         1   \n",
       "4               0  ...     0    0    0    0    0    0     0      0         1   \n",
       "...           ...  ...   ...  ...  ...  ...  ...  ...   ...    ...       ...   \n",
       "1576            0  ...     0    0    0    0    0    0     0      0         0   \n",
       "1577            0  ...     0    0    0    0    0    0     0      0         0   \n",
       "1578            0  ...     0    0    0    0    0    0     0      0         0   \n",
       "1579            0  ...     0    0    0    0    0    1     0      0         0   \n",
       "1580            0  ...     0    0    0    0    0    0     0      0         0   \n",
       "\n",
       "      yup  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "...   ...  \n",
       "1576    0  \n",
       "1577    0  \n",
       "1578    0  \n",
       "1579    0  \n",
       "1580    0  \n",
       "\n",
       "[1581 rows x 491 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow=pd.DataFrame(X,columns=features)\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the BOW data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('X_bow.npz',X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the BOW data into pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_bow,open('./df_bow.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applying it to the normalized corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581, 491)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf=tfidf.fit_transform(df['doc_tp']).toarray()\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the features after applying TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1581, 491)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=tfidf.get_feature_names()\n",
    "df_tfidf=pd.DataFrame(X_tfidf,columns=features)\n",
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the length of the features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving it in a pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_tfidf,open('./df_tfidf.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing required libraries for finding COSINE SIMILARITY between user query and database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating USER INTERFACE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Tell me something new\n",
      "System: I can help you work smarter instead of harder\n",
      "User: oops\n",
      "Could you ask in another way!!!\n",
      "User: sure\n",
      "System: Indeed.\n",
      "User: bye\n",
      "Bye,Hope we had a great talk!!!\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    Q=input('User: ')\n",
    "    if (Q=='Bye')or(Q=='bye'):\n",
    "        print('Bye,Hope we had a great talk!!!')\n",
    "        break\n",
    "    #step-1: Cleaning,Lemmatization and removing stop words\n",
    "    Q_lema=text_normalize(Q)\n",
    "    #step-2: Applying tfidf to user query\n",
    "    Q_tfidf=tfidf.transform([Q_lema]).toarray()\n",
    "    #step-3: FINDING COSINE SIMILARITY between database of queries and user query \n",
    "    #To get relevant response\n",
    "    cos=1-pairwise_distances(X_tfidf,Q_tfidf,metric='cosine')\n",
    "    ind=cos.argmax()\n",
    "    #step-4: Giving response to the query whose cosine value is greater than threshold \n",
    "    if ind > 0.2:\n",
    "        res=df.loc[df['Context']==df['Context'].loc[ind], 'Text Response']\n",
    "        print('System:',res.iloc[0])\n",
    "    else:\n",
    "        print('Could you ask in another way!!!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The chat will get terminated when user types 'bye'**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
